{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69da66bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/varalam/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/varalam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/varalam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import praw\n",
    "from datetime import datetime, timedelta\n",
    "from prawcore.exceptions import Redirect\n",
    "\n",
    "user_agent = \"Scraper 1.0 by AmphibianFinal9736\"\n",
    "reddit = praw.Reddit(client_id=\"HUmX-XMSMg7y2lnN-O21DA\", client_secret=\"e2w9HmBAsBOPwZvuHDDykxsm12PKIA\", user_agent=user_agent, password=\"Wangyibo@11\")\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "from prawcore.exceptions import Redirect\n",
    "\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "997e8535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_posts(subreddit_names, keywords, limit_per_subreddit=None):\n",
    "    sia = SIA()\n",
    "\n",
    "    # Lists to store sentiment scores\n",
    "    posts_results = []\n",
    "\n",
    "    for subreddit_name in subreddit_names:\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        print(f\"\\nFetching posts from r/{subreddit_name}...\\n{'=' * 40}\")\n",
    "\n",
    "        try:\n",
    "            for submission in subreddit.search(query=' '.join(keywords), sort='relevance', time_filter='all', limit=limit_per_subreddit):\n",
    "                # Perform sentiment analysis on the title\n",
    "                pol_score_post = sia.polarity_scores(submission.title)\n",
    "                pol_score_post['headline'] = submission.title\n",
    "\n",
    "                # Check if the submission has a self-text (body) and perform sentiment analysis\n",
    "                if submission.is_self and submission.selftext:\n",
    "                    pol_score_selftext = sia.polarity_scores(submission.selftext)\n",
    "                    pol_score_post['selftext'] = submission.selftext\n",
    "                    pol_score_post['selftext_sentiment'] = pol_score_selftext\n",
    "\n",
    "                posts_results.append(pol_score_post)\n",
    "\n",
    "        except Redirect as e:\n",
    "            print(f\"Redirect exception: {e}\")\n",
    "            continue\n",
    "\n",
    "    return posts_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0c48b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_headlines(df, additional_words_to_remove=None):\n",
    "    \"\"\"\n",
    "    Process headlines in a DataFrame by tokenizing, removing stopwords, punctuation, and specified words,\n",
    "    and then count the occurrences of each word. Also adds sentiment labels to the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing a 'headline' column.\n",
    "    - additional_words_to_remove: List of additional words to remove.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with columns 'Word', 'Frequency', and 'Label'.\n",
    "    \"\"\"\n",
    "    # Assign sentiment labels based on the 'compound' column\n",
    "    df['label'] = 0\n",
    "    df.loc[df['compound'] > 0.2, 'label'] = 1\n",
    "    df.loc[df['compound'] < -0.2, 'label'] = -1\n",
    "    \n",
    "    # Fill NaN values in the 'compound' column with 0\n",
    "    df['compound'] = df['compound'].fillna(0)\n",
    "\n",
    "    # Tokenize the comments into words\n",
    "    df['tokens'] = df['headline'].apply(word_tokenize)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['filtered_tokens'] = df['tokens'].apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words])\n",
    "\n",
    "    # Remove punctuation\n",
    "    df['filtered_tokens'] = df['filtered_tokens'].apply(lambda tokens: [word for word in tokens if word.isalnum()])\n",
    "\n",
    "    # Additional words to remove\n",
    "    words_to_remove = ['v12', 'sto', 'Lamborghini', 'Lambo', 'car', 'lambo', 'lamborghini', 'huracan', 'aventador', 'countach',\n",
    "                        'diablo', 'urus', 'cars', 'ferrari', 'revuelto', 'gallardo', 'miura', 'svj']\n",
    "    \n",
    "    \n",
    "    # Flatten the list of filtered tokens\n",
    "    all_words = [word for tokens in df['filtered_tokens'] for word in tokens]\n",
    "\n",
    "    # Count the occurrences of each word\n",
    "    word_counts = Counter(all_words)\n",
    "\n",
    "    # Get the most common words and their frequencies\n",
    "    processed_headlines = pd.DataFrame(word_counts.most_common(), columns=['Word', 'Frequency'])\n",
    "\n",
    "    # Add sentiment labels to the processed DataFrame\n",
    "    processed_headlines['Label'] = df['label']\n",
    "\n",
    "\n",
    "    if additional_words_to_remove:\n",
    "        words_to_remove.extend(additional_words_to_remove)\n",
    "\n",
    "    df['filtered_tokens'] = df['filtered_tokens'].apply(lambda tokens: [word.lower() for word in tokens if word.lower() not in words_to_remove])\n",
    "\n",
    "    \n",
    "    return processed_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3f54785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import os\n",
    "\n",
    "def generate_topics(df, subreddit_name, num_topics=3):\n",
    "    # List of Lamborghini car models to be removed\n",
    "    lamborghini_models = [\n",
    "        \"Huracan\", \"Aventador\", \"Urus\", \"Gallardo\", \"Murcielago\", \"Diablo\",\n",
    "        \"Countach\", \"Jalpa\", \"Espada\", \"Islero\", \"Jarama\", \"Silhouette\",\n",
    "        \"Urraco\", \"Miura\", \"Reventon\", \"Centenario\", \"Sian\", 'personam', 'Revuelto', 'svj'\n",
    "        # Add more Lamborghini models as need\n",
    "    ]\n",
    "\n",
    "    # Convert Lamborghini car models to lowercase\n",
    "    lamborghini_models_lower = [model.lower() for model in lamborghini_models]\n",
    "\n",
    "    # Tokenization and preprocessing\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    headline_doc = df['headline']\n",
    "    headline_doc = [word_tokenize(doc.lower()) for doc in headline_doc]\n",
    "    headline_doc = [\n",
    "        [word for word in doc if word.isalnum() and word not in stop_words and word not in lamborghini_models_lower]\n",
    "        for doc in headline_doc\n",
    "    ]\n",
    "\n",
    "    # Create TF-IDF matrix\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([' '.join(doc) for doc in headline_doc])\n",
    "\n",
    "    # Save TF-IDF matrix\n",
    "    save_path = f'{subreddit_name}_tfidf_matrix.npy'\n",
    "    np.save(save_path, tfidf_matrix)\n",
    "\n",
    "    # Apply Truncated SVD (LSA)\n",
    "    lsa_model = TruncatedSVD(n_components=num_topics)\n",
    "    lsa_topic_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "    # Print the top words for each topic\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    topics = {}\n",
    "    for i in range(num_topics):\n",
    "        top_words_idx = lsa_model.components_[i].argsort()[-5:][::-1]\n",
    "        top_words = [terms[idx] for idx in top_words_idx]\n",
    "        topics[f'Topic {i + 1}'] = top_words\n",
    "\n",
    "    # Return the identifier along with topics and tfidf_matrix\n",
    "    return subreddit_name, (topics, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "781387cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "\n",
    "def train_lda_model(documents, num_topics=10, num_words=5, passes=10):\n",
    "    # Tokenization and preprocessing\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    documents = [word_tokenize(doc.lower()) for doc in documents]\n",
    "    documents = [[word for word in doc if word.isalnum() and word not in stop_words] for doc in documents]\n",
    "\n",
    "    # Create a dictionary representation of the documents\n",
    "    dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "    # Create a corpus from the documents\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    # Train the LDA model\n",
    "    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=passes)\n",
    "\n",
    "    # Print the topics\n",
    "    topics = lda_model.print_topics(num_words=num_words)\n",
    "    formatted_topics = []\n",
    "\n",
    "    for topic_id, topic in topics:\n",
    "        terms = [term.split('\"')[1] for term in topic.split('+')]\n",
    "        formatted_topic = f\"Topic {topic_id}: {' + '.join(terms)}\"\n",
    "        formatted_topics.append(formatted_topic)\n",
    "\n",
    "    return formatted_topics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c15f1efb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching posts from r/lamborghini...\n",
      "========================================\n",
      "Posts DataFrame:\n",
      "   neg    neu    pos  compound  \\\n",
      "0  0.0  0.734  0.266    0.4404   \n",
      "1  0.0  0.769  0.231    0.3400   \n",
      "2  0.0  1.000  0.000    0.0000   \n",
      "3  0.0  1.000  0.000    0.0000   \n",
      "4  0.0  1.000  0.000    0.0000   \n",
      "5  0.0  1.000  0.000    0.0000   \n",
      "6  0.0  1.000  0.000    0.0000   \n",
      "\n",
      "                                            headline  \\\n",
      "0  Retro Racing Game - World Record Seems Easy to...   \n",
      "1  Any of you play (sim) racing games with Lambor...   \n",
      "2  Asphalt 8: Airborne Part 162 | Lamborghini Ven...   \n",
      "3                                       Need to khow   \n",
      "4                     Exploring Lambos in Videogames   \n",
      "5  Lamborghini Jarama 1970 - Ferruccio Lamborghin...   \n",
      "6    Made myself a Lamborghini chair for sim racing.   \n",
      "\n",
      "                                            selftext  \\\n",
      "0  My buddy made a simple retro racing game and s...   \n",
      "1  I myself play Assetto Corsa Competizione where...   \n",
      "2                                                NaN   \n",
      "3  I want to buy a lamborghini but there is a pro...   \n",
      "4  Hello Lambo Community. I recently found this s...   \n",
      "5                                                NaN   \n",
      "6                                                NaN   \n",
      "\n",
      "                                  selftext_sentiment  \n",
      "0  {'neg': 0.063, 'neu': 0.779, 'pos': 0.158, 'co...  \n",
      "1  {'neg': 0.0, 'neu': 0.704, 'pos': 0.296, 'comp...  \n",
      "2                                                NaN  \n",
      "3  {'neg': 0.065, 'neu': 0.855, 'pos': 0.08, 'com...  \n",
      "4  {'neg': 0.037, 'neu': 0.785, 'pos': 0.178, 'co...  \n",
      "5                                                NaN  \n",
      "6                                                NaN  \n",
      "            Word  Frequency  Label\n",
      "0    Lamborghini          5    1.0\n",
      "1         Racing          2    1.0\n",
      "2            sim          2    0.0\n",
      "3         racing          2    0.0\n",
      "4          Retro          1    0.0\n",
      "5           Game          1    0.0\n",
      "6          World          1    0.0\n",
      "7         Record          1    NaN\n",
      "8          Seems          1    NaN\n",
      "9           Easy          1    NaN\n",
      "10          Beat          1    NaN\n",
      "11          play          1    NaN\n",
      "12         games          1    NaN\n",
      "13  Lamborghinis          1    NaN\n",
      "14       Asphalt          1    NaN\n",
      "15             8          1    NaN\n",
      "16      Airborne          1    NaN\n",
      "17          Part          1    NaN\n",
      "18           162          1    NaN\n",
      "19        Veneno          1    NaN\n",
      "20        London          1    NaN\n",
      "21           LVH          1    NaN\n",
      "22         Games          1    NaN\n",
      "23          Need          1    NaN\n",
      "24          khow          1    NaN\n",
      "25     Exploring          1    NaN\n",
      "26        Lambos          1    NaN\n",
      "27    Videogames          1    NaN\n",
      "28        Jarama          1    NaN\n",
      "29          1970          1    NaN\n",
      "30     Ferruccio          1    NaN\n",
      "31        bought          1    NaN\n",
      "32           one          1    NaN\n",
      "33         brand          1    NaN\n",
      "34           new          1    NaN\n",
      "35         owned          1    NaN\n",
      "36       company          1    NaN\n",
      "37           car          1    NaN\n",
      "38        choice          1    NaN\n",
      "39          Made          1    NaN\n",
      "40         chair          1    NaN\n",
      "Topic 1: ['lamborghini', 'racing', 'sim', 'games', 'chair']\n",
      "Topic 2: ['lamborghini', 'bought', 'company', 'ferruccio', 'choice']\n",
      "Topic 3: ['exploring', 'videogames', 'lambos', 'need', 'khow']\n",
      "Topic 0: racing + need + exploring + khow + lamborghini\n",
      "Topic 1: racing + exploring + lamborghini + need + sim\n",
      "Topic 2: lamborghini + brand + bought + jarama + 1970\n",
      "Topic 3: racing + khow + need + exploring + lamborghini\n",
      "Topic 4: racing + seems + beat + game + world\n",
      "Topic 5: racing + games + veneno + london + part\n",
      "Topic 6: need + khow + lamborghini + sim + racing\n",
      "Topic 7: sim + play + lamborghinis + games + racing\n",
      "Topic 8: made + chair + lambos + videogames + exploring\n",
      "Topic 9: racing + khow + lamborghini + exploring + need\n"
     ]
    }
   ],
   "source": [
    "# Set the subreddit and keyword\n",
    "subreddit_names = ['lamborghini']\n",
    "keywords = ['games', 'game', 'GAME', 'videogames', 'racinggame', 'video games', 'raced', 'race', 'league', 'tournament', 'collab', 'collaboration', 'asia', 'singapore', 'Singapore']\n",
    "\n",
    "# Call the function and get the results\n",
    "posts_results = fetch_posts(subreddit_names, keywords, limit_per_subreddit=None)\n",
    "\n",
    "# Create a data frame\n",
    "posts_df = pd.DataFrame.from_records(posts_results)\n",
    "\n",
    "# Print the data frame\n",
    "print(\"Posts DataFrame:\")\n",
    "print(posts_df)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a DataFrame named 'headlines_df'\n",
    "processed_headlines_df = process_headlines(posts_df, additional_words_to_remove=['example', 'additional'])\n",
    "\n",
    "# Display the processed DataFrame\n",
    "print(processed_headlines_df.head(50))\n",
    "\n",
    "# Truncated SVD (LSA) model:\n",
    "# Assuming generate_topics returns a tuple with a string (subreddit name) and a dictionary of topics\n",
    "result = generate_topics(posts_df, subreddit_name='lamborghini')\n",
    "topics, tfidf_matrix = result[1]  # Access the second element of the tuple\n",
    "\n",
    "# Iterate over topics\n",
    "for topic, top_words in topics.items():\n",
    "    print(f'{topic}: {top_words}')\n",
    "\n",
    "\n",
    "# LDA model:\n",
    "headline_topics = train_lda_model(posts_df['headline'])\n",
    "for topic in headline_topics:\n",
    "    print(topic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f233a9bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching posts from r/askSingapore...\n",
      "========================================\n",
      "Posts DataFrame:\n",
      "     neg    neu    pos  compound  \\\n",
      "0    0.0  1.000  0.000    0.0000   \n",
      "1    0.0  1.000  0.000    0.0000   \n",
      "2    0.0  1.000  0.000    0.0000   \n",
      "3    0.0  1.000  0.000    0.0000   \n",
      "4    0.0  1.000  0.000    0.0000   \n",
      "..   ...    ...    ...       ...   \n",
      "231  0.0  0.674  0.326    0.4404   \n",
      "232  0.0  1.000  0.000    0.0000   \n",
      "233  0.0  1.000  0.000    0.0000   \n",
      "234  0.0  1.000  0.000    0.0000   \n",
      "235  0.0  0.385  0.615    0.4939   \n",
      "\n",
      "                                              headline  \\\n",
      "0                     Will Singapore be your end game?   \n",
      "1                             Dating game in Singapore   \n",
      "2                Video game beta testers in Singapore?   \n",
      "3                Game development courses in Singapore   \n",
      "4    If Singaporeâ€™s education system were a game, w...   \n",
      "..                                                 ...   \n",
      "231        Need advice on getting better NS vocations.   \n",
      "232                     What should I do with my life?   \n",
      "233         Where can I find old magazine in Singapore   \n",
      "234                                  Audio engineering   \n",
      "235                         Meeting Singaporean Friend   \n",
      "\n",
      "                                              selftext  \\\n",
      "0    More and more, as years go by, Singapore is no...   \n",
      "1    32 years old non existent dating life here.:( ...   \n",
      "2    Hi everyone,\\n\\nIâ€™m just wondering if anyone k...   \n",
      "3    \\nIâ€™m currently thinking about NYP game develo...   \n",
      "4    It would be cool if we treated studying as fun...   \n",
      "..                                                 ...   \n",
      "231  (Couldn't post on r/Singapore) I have been dre...   \n",
      "232  I am in secondary 4 this year express stream. ...   \n",
      "233  Hi all, will like to find out where can I buy ...   \n",
      "234  Hi, I am a 25F who graduated in university a y...   \n",
      "235  A friend of mine from Singapore is coming to t...   \n",
      "\n",
      "                                    selftext_sentiment  \n",
      "0    {'neg': 0.105, 'neu': 0.852, 'pos': 0.043, 'co...  \n",
      "1    {'neg': 0.112, 'neu': 0.767, 'pos': 0.122, 'co...  \n",
      "2    {'neg': 0.0, 'neu': 0.729, 'pos': 0.271, 'comp...  \n",
      "3    {'neg': 0.0, 'neu': 0.839, 'pos': 0.161, 'comp...  \n",
      "4    {'neg': 0.0, 'neu': 0.641, 'pos': 0.359, 'comp...  \n",
      "..                                                 ...  \n",
      "231  {'neg': 0.14, 'neu': 0.788, 'pos': 0.072, 'com...  \n",
      "232  {'neg': 0.131, 'neu': 0.733, 'pos': 0.136, 'co...  \n",
      "233  {'neg': 0.084, 'neu': 0.786, 'pos': 0.13, 'com...  \n",
      "234  {'neg': 0.03, 'neu': 0.899, 'pos': 0.071, 'com...  \n",
      "235  {'neg': 0.085, 'neu': 0.78, 'pos': 0.135, 'com...  \n",
      "\n",
      "[236 rows x 7 columns]\n",
      "            Word  Frequency  Label\n",
      "0      Singapore         82    0.0\n",
      "1          games         22    0.0\n",
      "2           game         20    0.0\n",
      "3        Looking         13    0.0\n",
      "4            buy         12    0.0\n",
      "5         gaming         12    1.0\n",
      "6             SG         12    0.0\n",
      "7             PC         11    0.0\n",
      "8    Singaporean          9    0.0\n",
      "9           find          8    1.0\n",
      "10         watch          7    0.0\n",
      "11        laptop          7    0.0\n",
      "12      football          6    0.0\n",
      "13        Gaming          6    0.0\n",
      "14          Game          5    0.0\n",
      "15         video          5    0.0\n",
      "16     singapore          5    0.0\n",
      "17        anyone          5    0.0\n",
      "18        people          5    0.0\n",
      "19  Singaporeans          5    1.0\n",
      "20          play          5    0.0\n",
      "21        advice          5    0.0\n",
      "22          Need          5    0.0\n",
      "23       popular          5    0.0\n",
      "24      overseas          5    0.0\n",
      "25        gamers          5    0.0\n",
      "26          like          5    0.0\n",
      "27     community          5    0.0\n",
      "28         would          4    0.0\n",
      "29         alone          4    0.0\n",
      "30        online          4   -1.0\n",
      "31        Anyone          4    0.0\n",
      "32          Help          4    1.0\n",
      "33            US          4    0.0\n",
      "34        really          4    0.0\n",
      "35       friends          4    0.0\n",
      "36          card          4    0.0\n",
      "37          know          4    0.0\n",
      "38    Basketball          4    0.0\n",
      "39        arcade          3    0.0\n",
      "40          want          3   -1.0\n",
      "41        social          3    1.0\n",
      "42          best          3    1.0\n",
      "43       playing          3    0.0\n",
      "44        Laptop          3    1.0\n",
      "45    interested          3    0.0\n",
      "46      Football          3    0.0\n",
      "47         leave          3    1.0\n",
      "48       culture          3    0.0\n",
      "49         think          3    1.0\n",
      "Topic 1: ['singapore', 'game', 'gaming', 'games', 'buy']\n",
      "Topic 2: ['gaming', 'pc', 'buy', 'laptop', 'building']\n",
      "Topic 3: ['games', 'football', 'pc', 'gaming', 'buy']\n",
      "Topic 0: singaporean + pc + soon + life + days\n",
      "Topic 1: singapore + games + need + really + help\n",
      "Topic 2: singapore + game + arcade + find + around\n",
      "Topic 3: singapore + us + games + basketball + sg\n",
      "Topic 4: singapore + gaming + buy + laptop + sg\n",
      "Topic 5: singapore + looking + gamers + community + play\n",
      "Topic 6: singapore + game + anyone + sg + come\n",
      "Topic 7: singapore + games + football + singaporean + anyone\n",
      "Topic 8: singapore + game + find + best + still\n",
      "Topic 9: stay + sports + considered + life + singapore\n"
     ]
    }
   ],
   "source": [
    "# Set the subreddit and keyword\n",
    "subreddit_names = ['askSingapore']\n",
    "keywords = ['games', 'game', 'GAME', 'videogames', 'racinggame', 'video games', 'raced', 'race', 'league', 'tournament', 'collab', 'collaboration', 'asia', 'singapore', 'Singapore']\n",
    "\n",
    "# Call the function and get the results\n",
    "posts_results = fetch_posts(subreddit_names, keywords, limit_per_subreddit=None)\n",
    "\n",
    "# Create a data frame\n",
    "posts_df = pd.DataFrame.from_records(posts_results)\n",
    "\n",
    "# Print the data frame\n",
    "print(\"Posts DataFrame:\")\n",
    "print(posts_df)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a DataFrame named 'headlines_df'\n",
    "processed_headlines_df = process_headlines(posts_df, additional_words_to_remove=['example', 'additional'])\n",
    "\n",
    "# Display the processed DataFrame\n",
    "print(processed_headlines_df.head(50))\n",
    "\n",
    "# Truncated SVD (LSA) model:\n",
    "# Assuming generate_topics returns a tuple with a string (subreddit name) and a dictionary of topics\n",
    "result = generate_topics(posts_df, subreddit_name='lamborghini')\n",
    "topics, tfidf_matrix = result[1]  # Access the second element of the tuple\n",
    "\n",
    "# Iterate over topics\n",
    "for topic, top_words in topics.items():\n",
    "    print(f'{topic}: {top_words}')\n",
    "\n",
    "\n",
    "# LDA model:\n",
    "headline_topics = train_lda_model(posts_df['headline'])\n",
    "for topic in headline_topics:\n",
    "    print(topic)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51fdaaee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching posts from r/simracing...\n",
      "========================================\n",
      "Posts DataFrame:\n",
      "       neg    neu    pos  compound  \\\n",
      "0    0.000  1.000  0.000    0.0000   \n",
      "1    0.000  1.000  0.000    0.0000   \n",
      "2    0.000  1.000  0.000    0.0000   \n",
      "3    0.134  0.642  0.225    0.4019   \n",
      "4    0.000  0.804  0.196    0.4449   \n",
      "..     ...    ...    ...       ...   \n",
      "174  0.000  0.462  0.538    0.7906   \n",
      "175  0.000  1.000  0.000    0.0000   \n",
      "176  0.000  1.000  0.000    0.0000   \n",
      "177  0.000  1.000  0.000    0.0000   \n",
      "178  0.000  0.879  0.121    0.0258   \n",
      "\n",
      "                                              headline  \\\n",
      "0    How my perspective on simracing changed after ...   \n",
      "1                            NEW Lamborghini SC24 LMDH   \n",
      "2    My First Ever Racing Game - Automobili Lamborg...   \n",
      "3    Best sounding car in the gameâ€¦ Lamborghini R-G...   \n",
      "4    Cant have shit in Detroit .They took my Lambor...   \n",
      "..                                                 ...   \n",
      "174    The Best Sims For Various Popular Racing Series   \n",
      "175          Assetto Corsa Competizione 1.0 is out now   \n",
      "176  AC vs. ACC vs. rF2 vs. AMS2 vs. PC2 vs. iRacin...   \n",
      "177                  Yeahâ€¦. GT7 physics is on point ðŸ¤¥ðŸ˜‚   \n",
      "178  Your average public lobby (Super Trofeo start ...   \n",
      "\n",
      "                                              selftext  \\\n",
      "0    I've been simracing for the better part of 7 y...   \n",
      "1                                                  NaN   \n",
      "2                                                  NaN   \n",
      "3                                                  NaN   \n",
      "4                                                  NaN   \n",
      "..                                                 ...   \n",
      "174  I own a fair amount of current gen racing sims...   \n",
      "175  ACC 1.0 has been released now. 5.5GB download....   \n",
      "176  **Edit: Added RaceRoom (R3E)!**\\n\\n# Backgroun...   \n",
      "177                                                NaN   \n",
      "178                                                NaN   \n",
      "\n",
      "                                    selftext_sentiment  \n",
      "0    {'neg': 0.054, 'neu': 0.86, 'pos': 0.086, 'com...  \n",
      "1                                                  NaN  \n",
      "2                                                  NaN  \n",
      "3                                                  NaN  \n",
      "4                                                  NaN  \n",
      "..                                                 ...  \n",
      "174  {'neg': 0.063, 'neu': 0.747, 'pos': 0.19, 'com...  \n",
      "175  {'neg': 0.05, 'neu': 0.864, 'pos': 0.086, 'com...  \n",
      "176  {'neg': 0.057, 'neu': 0.795, 'pos': 0.148, 'co...  \n",
      "177                                                NaN  \n",
      "178                                                NaN  \n",
      "\n",
      "[179 rows x 7 columns]\n",
      "            Word  Frequency  Label\n",
      "0    Lamborghini         86    0.0\n",
      "1        Assetto         31    0.0\n",
      "2          Corsa         29    0.0\n",
      "3        Huracan         24    1.0\n",
      "4            GT3         20    1.0\n",
      "5      Challenge         20    0.0\n",
      "6         Weekly         19    0.0\n",
      "7           Week         17    1.0\n",
      "8           2019         16    0.0\n",
      "9            ACC         12    0.0\n",
      "10             2         12    0.0\n",
      "11      December         12    0.0\n",
      "12          2018         12    0.0\n",
      "13            VR         10    0.0\n",
      "14        Racing          9    0.0\n",
      "15   Nurburgring          9    0.0\n",
      "16           sim          8    0.0\n",
      "17         wheel          7    0.0\n",
      "18     Aventador          7    0.0\n",
      "19          cars          7    0.0\n",
      "20           car          6   -1.0\n",
      "21  Competizione          6    0.0\n",
      "22            GT          6    1.0\n",
      "23           new          6    0.0\n",
      "24        racing          6    0.0\n",
      "25         Super          6    1.0\n",
      "26       iRacing          6    0.0\n",
      "27       Project          6    0.0\n",
      "28         still          6    0.0\n",
      "29    Challenges          6    0.0\n",
      "30       Diamond          6    1.0\n",
      "31         Track          5    0.0\n",
      "32           Lap          5    1.0\n",
      "33       Porsche          5    1.0\n",
      "34  Nordschleife          5   -1.0\n",
      "35       Ferrari          5    0.0\n",
      "36            SV          5    0.0\n",
      "37           get          5    0.0\n",
      "38          guys          5   -1.0\n",
      "39          game          5    0.0\n",
      "40          Cars          5    0.0\n",
      "41         First          4    0.0\n",
      "42            vs          4    0.0\n",
      "43          Race          4    0.0\n",
      "44         first          4    0.0\n",
      "45        Veneno          4    0.0\n",
      "46           1st          4    1.0\n",
      "47          race          4    0.0\n",
      "48          made          4    1.0\n",
      "49           New          4    0.0\n",
      "Topic 1: ['corsa', 'assetto', 'lamborghini', 'sv', 'gt3']\n",
      "Topic 2: ['weekly', 'challenge', 'week', '2019', '2018']\n",
      "Topic 3: ['december', '2018', 'diamond', 'challenges', '17th']\n",
      "Topic 0: racing + sim + lamborghini + acc + cars\n",
      "Topic 1: lamborghini + gt3 + huracan + lap + nurburgring\n",
      "Topic 2: lamborghini + corsa + assetto + sv + nurburgring\n",
      "Topic 3: lamborghini + game + cars + acc + setup\n",
      "Topic 4: lamborghini + new + first + racing + race\n",
      "Topic 5: lamborghini + corsa + assetto + huracan + competizione\n",
      "Topic 6: lamborghini + endurance + may + gt3 + super\n",
      "Topic 7: challenge + weekly + week + 2019 + december\n",
      "Topic 8: lamborghini + vr + gt + ai + guys\n",
      "Topic 9: lamborghini + wheel + changed + questions + gt1\n"
     ]
    }
   ],
   "source": [
    "# Set the subreddit and keyword\n",
    "subreddit_names = ['simracing']\n",
    "keywords = ['lamborghini', 'Lamborghini', 'Lambo', 'lambo', 'LAMBORGHINI', 'singapore', 'Singapore']\n",
    "# Call the function and get the results\n",
    "posts_results = fetch_posts(subreddit_names, keywords, limit_per_subreddit=None)\n",
    "\n",
    "# Create a data frame\n",
    "posts_df = pd.DataFrame.from_records(posts_results)\n",
    "\n",
    "# Print the data frame\n",
    "print(\"Posts DataFrame:\")\n",
    "print(posts_df)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a DataFrame named 'headlines_df'\n",
    "processed_headlines_df = process_headlines(posts_df, additional_words_to_remove=['example', 'additional'])\n",
    "\n",
    "# Display the processed DataFrame\n",
    "print(processed_headlines_df.head(50))\n",
    "\n",
    "# Truncated SVD (LSA) model:\n",
    "# Assuming generate_topics returns a tuple with a string (subreddit name) and a dictionary of topics\n",
    "result = generate_topics(posts_df, subreddit_name='simracing')\n",
    "topics, tfidf_matrix = result[1]  # Access the second element of the tuple\n",
    "\n",
    "# Iterate over topics\n",
    "for topic, top_words in topics.items():\n",
    "    print(f'{topic}: {top_words}')\n",
    "\n",
    "\n",
    "\n",
    "# LDA model:\n",
    "headline_topics = train_lda_model(posts_df['headline'])\n",
    "for topic in headline_topics:\n",
    "    print(topic)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d29926d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching posts from r/rocketracing...\n",
      "========================================\n",
      "Posts DataFrame:\n",
      "      neg    neu    pos  compound  \\\n",
      "0   0.000  1.000  0.000    0.0000   \n",
      "1   0.000  0.826  0.174    0.2263   \n",
      "2   0.000  1.000  0.000    0.0000   \n",
      "3   0.000  1.000  0.000    0.0000   \n",
      "4   0.000  0.612  0.388    0.5859   \n",
      "5   0.086  0.914  0.000   -0.3089   \n",
      "6   0.000  1.000  0.000    0.0000   \n",
      "7   0.182  0.818  0.000   -0.3971   \n",
      "8   0.000  1.000  0.000    0.0000   \n",
      "9   0.000  1.000  0.000    0.0000   \n",
      "10  0.000  1.000  0.000    0.0000   \n",
      "11  0.234  0.660  0.107   -0.5423   \n",
      "12  0.000  1.000  0.000    0.0000   \n",
      "13  0.368  0.632  0.000   -0.5413   \n",
      "14  0.000  1.000  0.000    0.0000   \n",
      "15  0.000  0.702  0.298    0.5256   \n",
      "16  0.000  0.556  0.444    0.3400   \n",
      "17  0.000  1.000  0.000    0.0000   \n",
      "18  0.000  0.797  0.203    0.4215   \n",
      "19  0.000  1.000  0.000    0.0000   \n",
      "20  0.000  1.000  0.000    0.0000   \n",
      "21  0.000  1.000  0.000    0.0000   \n",
      "22  0.000  0.826  0.174    0.2732   \n",
      "23  0.000  1.000  0.000    0.0000   \n",
      "24  0.000  1.000  0.000    0.0000   \n",
      "25  0.000  0.738  0.262    0.4927   \n",
      "26  0.000  1.000  0.000    0.0000   \n",
      "27  0.000  0.597  0.403    0.4019   \n",
      "28  0.000  1.000  0.000    0.0000   \n",
      "29  0.000  1.000  0.000    0.0000   \n",
      "30  0.000  1.000  0.000    0.0000   \n",
      "\n",
      "                                             headline  \\\n",
      "0   Anyone knows when will the lamborghini be avai...   \n",
      "1   Is the Lamborghini pack worth it in the item s...   \n",
      "2        Will they make Lamborghini available again ?   \n",
      "3                  How do I make my Lamborghini black   \n",
      "4       First game with Lamborghini Huracan and I win   \n",
      "5   Just so people are aware. Epic are literally c...   \n",
      "6   My custom made Always on display for phone ft ...   \n",
      "7   To everyone who are complaining about the car ...   \n",
      "8   on this race there was a Octane using the Lamb...   \n",
      "9   The Lamborghini HuracÃ¡n STO Bundle includes a ...   \n",
      "10  \"The first Car Bodies to get the Cross-Game tr...   \n",
      "11  Eyy check it out, the lambo made a surprise re...   \n",
      "12                        Where are my fellow Lambos?   \n",
      "13    The most disappointing part about Rocket Racing   \n",
      "14                                          How much?   \n",
      "15  Rocket Racing shop I designed (I'm Very proud ...   \n",
      "16                              My races in Diamond ðŸ’Ž   \n",
      "17  I saw players with different rims but canâ€™t fi...   \n",
      "18  Hoping they add all the bodies to fortnite and...   \n",
      "19              Will they add DLC cars youâ€™ve bought?   \n",
      "20                           Edging closer to unreal!   \n",
      "21  Does anyone else match their skin with the car...   \n",
      "22  Well, my Black JÃ¤ger 619 and Titanium Octane a...   \n",
      "23                                     whey...what? ðŸ˜¯   \n",
      "24                 Rocket League cars wonâ€™t transfer.   \n",
      "25  I really hope we get a proper cross inventory ...   \n",
      "26  I havenâ€™t gotten anything from my rocket leagu...   \n",
      "27                         Hopefully we get more cars   \n",
      "28        What cars transfer over from rocket league?   \n",
      "29  Do you guys think the Silvia or other BP cars ...   \n",
      "30                                             Huh. ðŸ˜¶   \n",
      "\n",
      "                                             selftext  \\\n",
      "0   I was waiting till the end of January to buy i...   \n",
      "1      How many of u actually own that. So expensive.   \n",
      "2   It's weird one of the only cars they're adding...   \n",
      "3   I canâ€™t seem to find the black colour for my L...   \n",
      "4   I don't know what it is, maybe it's ego boost,...   \n",
      "5                                                 NaN   \n",
      "6                                                 NaN   \n",
      "7                                                 NaN   \n",
      "8                                                 NaN   \n",
      "9                                                 NaN   \n",
      "10  From today's blog post: \\n\\nhttps://www.reddit...   \n",
      "11                                                NaN   \n",
      "12  I have not seen a single other person using th...   \n",
      "13  The most disappointing part about rocket racin...   \n",
      "14  How much is the Lamborghini bundle? Mine shows...   \n",
      "15                                                NaN   \n",
      "16                                                NaN   \n",
      "17  as i was waiting to start a race i was scrolli...   \n",
      "18  I use animus GP and insidio alot in normal roc...   \n",
      "19  I bought the McLaren awhile back on Xbox rocke...   \n",
      "20                                                NaN   \n",
      "21                                                NaN   \n",
      "22                                                NaN   \n",
      "23                                                NaN   \n",
      "24  My epic, rocket league and playstation account...   \n",
      "25  I stopped caring about Rocket League a while a...   \n",
      "26  Seems like others are getting things from thei...   \n",
      "27  Even if we donâ€™T get all I really hope the twi...   \n",
      "28  I want to know what cars will transfer and if ...   \n",
      "29  Or will epic be leaving them out due to licens...   \n",
      "30                                                NaN   \n",
      "\n",
      "                                   selftext_sentiment  \n",
      "0   {'neg': 0.225, 'neu': 0.775, 'pos': 0.0, 'comp...  \n",
      "1   {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  \n",
      "2   {'neg': 0.101, 'neu': 0.835, 'pos': 0.064, 'co...  \n",
      "3   {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  \n",
      "4   {'neg': 0.0, 'neu': 0.545, 'pos': 0.455, 'comp...  \n",
      "5                                                 NaN  \n",
      "6                                                 NaN  \n",
      "7                                                 NaN  \n",
      "8                                                 NaN  \n",
      "9                                                 NaN  \n",
      "10  {'neg': 0.007, 'neu': 0.963, 'pos': 0.03, 'com...  \n",
      "11                                                NaN  \n",
      "12  {'neg': 0.077, 'neu': 0.923, 'pos': 0.0, 'comp...  \n",
      "13  {'neg': 0.132, 'neu': 0.781, 'pos': 0.087, 'co...  \n",
      "14  {'neg': 0.0, 'neu': 0.789, 'pos': 0.211, 'comp...  \n",
      "15                                                NaN  \n",
      "16                                                NaN  \n",
      "17  {'neg': 0.0, 'neu': 0.938, 'pos': 0.062, 'comp...  \n",
      "18  {'neg': 0.0, 'neu': 0.807, 'pos': 0.193, 'comp...  \n",
      "19  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  \n",
      "20                                                NaN  \n",
      "21                                                NaN  \n",
      "22                                                NaN  \n",
      "23                                                NaN  \n",
      "24  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  \n",
      "25  {'neg': 0.088, 'neu': 0.628, 'pos': 0.284, 'co...  \n",
      "26  {'neg': 0.0, 'neu': 0.878, 'pos': 0.122, 'comp...  \n",
      "27  {'neg': 0.0, 'neu': 0.775, 'pos': 0.225, 'comp...  \n",
      "28  {'neg': 0.0, 'neu': 0.936, 'pos': 0.064, 'comp...  \n",
      "29  {'neg': 0.0, 'neu': 0.901, 'pos': 0.099, 'comp...  \n",
      "30                                                NaN  \n",
      "           Word  Frequency  Label\n",
      "0   Lamborghini          9    0.0\n",
      "1          cars          5    1.0\n",
      "2           get          4    0.0\n",
      "3           STO          3    0.0\n",
      "4        Octane          3    1.0\n",
      "5        Rocket          3   -1.0\n",
      "6        rocket          3    0.0\n",
      "7          shop          2   -1.0\n",
      "8          make          2    0.0\n",
      "9       Huracan          2    0.0\n",
      "10         made          2    0.0\n",
      "11          car          2   -1.0\n",
      "12         race          2    0.0\n",
      "13       wheels          2   -1.0\n",
      "14     possible          2    0.0\n",
      "15        check          2    1.0\n",
      "16       anyone          2    1.0\n",
      "17      Cyclone          2    0.0\n",
      "18      HuracÃ¡n          2    1.0\n",
      "19          Car          2    0.0\n",
      "20       Wheels          2    0.0\n",
      "21        JÃ¤ger          2    0.0\n",
      "22          619          2    1.0\n",
      "23       Racing          2    0.0\n",
      "24          add          2    0.0\n",
      "25     transfer          2    1.0\n",
      "26       league          2    0.0\n",
      "27       Anyone          1    1.0\n",
      "28        knows          1    0.0\n",
      "29  lamborghini          1    0.0\n",
      "30      avaible          1    0.0\n",
      "31         pack          1    NaN\n",
      "32        worth          1    NaN\n",
      "33         item          1    NaN\n",
      "34    available          1    NaN\n",
      "35        black          1    NaN\n",
      "36        First          1    NaN\n",
      "37         game          1    NaN\n",
      "38          win          1    NaN\n",
      "39       people          1    NaN\n",
      "40        aware          1    NaN\n",
      "41         Epic          1    NaN\n",
      "42    literally          1    NaN\n",
      "43     charging          1    NaN\n",
      "44         half          1    NaN\n",
      "45     contents          1    NaN\n",
      "46       bundle          1    NaN\n",
      "47         cost          1    NaN\n",
      "48           3k          1    NaN\n",
      "49    September          1    NaN\n",
      "Topic 1: ['rocket', 'cars', 'league', 'transfer', 'racing']\n",
      "Topic 2: ['lamborghini', 'make', 'available', 'black', 'wheels']\n",
      "Topic 3: ['racing', 'rocket', 'shop', 'disappointing', 'part']\n",
      "Topic 0: lamborghini + wheels + race + possible + octane\n",
      "Topic 1: rocket + cars + get + league + transfer\n",
      "Topic 2: lamborghini + available + make + items + get\n",
      "Topic 3: car + shop + available + sto + types\n",
      "Topic 4: much + whey + lamborghini + rocket + cars\n",
      "Topic 5: made + check + hell + expensive + still\n",
      "Topic 6: bundle + let + half + charging + epic\n",
      "Topic 7: lamborghini + make + huracan + anyone + game\n",
      "Topic 8: black + fortnite + octane + well + appeared\n",
      "Topic 9: racing + shop + rocket + designed + add\n"
     ]
    }
   ],
   "source": [
    "# Set the subreddit and keyword\n",
    "subreddit_names = ['rocketracing']\n",
    "keywords = ['lamborghini', 'Lamborghini', 'Lambo', 'lambo', 'LAMBORGHINI', 'singapore', 'Singapore']\n",
    "# Call the function and get the results\n",
    "posts_results = fetch_posts(subreddit_names, keywords, limit_per_subreddit=None)\n",
    "\n",
    "# Create a data frame\n",
    "posts_df = pd.DataFrame.from_records(posts_results)\n",
    "\n",
    "# Print the data frame\n",
    "print(\"Posts DataFrame:\")\n",
    "print(posts_df)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a DataFrame named 'headlines_df'\n",
    "processed_headlines_df = process_headlines(posts_df, additional_words_to_remove=['lamborghini', 'Lamborghini', 'Lambo', 'lambo', 'LAMBORGHINI', 'livery'])\n",
    "\n",
    "# Display the processed DataFrame\n",
    "print(processed_headlines_df.head(50))\n",
    "\n",
    "# Truncated SVD (LSA) model:\n",
    "# Assuming generate_topics returns a tuple with a string (subreddit name) and a dictionary of topics\n",
    "result = generate_topics(posts_df, subreddit_name='simracing')\n",
    "topics, tfidf_matrix = result[1]  # Access the second element of the tuple\n",
    "\n",
    "# Iterate over topics\n",
    "for topic, top_words in topics.items():\n",
    "    print(f'{topic}: {top_words}')\n",
    "\n",
    "\n",
    "\n",
    "# LDA model:\n",
    "headline_topics = train_lda_model(posts_df['headline'])\n",
    "for topic in headline_topics:\n",
    "    print(topic)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "daa86932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching posts from r/forza...\n",
      "========================================\n",
      "Posts DataFrame:\n",
      "     neg  neu  pos  compound  \\\n",
      "0    0.0  1.0  0.0       0.0   \n",
      "1    0.0  1.0  0.0       0.0   \n",
      "2    0.0  1.0  0.0       0.0   \n",
      "3    0.0  1.0  0.0       0.0   \n",
      "4    0.0  1.0  0.0       0.0   \n",
      "..   ...  ...  ...       ...   \n",
      "230  0.0  1.0  0.0       0.0   \n",
      "231  0.0  1.0  0.0       0.0   \n",
      "232  0.0  1.0  0.0       0.0   \n",
      "233  0.0  1.0  0.0       0.0   \n",
      "234  0.0  1.0  0.0       0.0   \n",
      "\n",
      "                                              headline selftext  \\\n",
      "0    Do we think the new Lamborghini Countach will ...      NaN   \n",
      "1                        Lamborghini Urus acceleration      NaN   \n",
      "2    The Evolution of the Forza Horizon Garages wit...      NaN   \n",
      "3    [Wishlist] Extraterrestrial Lamborghini Counta...      NaN   \n",
      "4                     Opinion on the Lamborghini Sian?      NaN   \n",
      "..                                                 ...      ...   \n",
      "230  When was the 2016 Lamborghini Aventador LP 750...      NaN   \n",
      "231         The legendary lime green Lamborghini Miura      NaN   \n",
      "232  How my perspective on simracing changed after ...      NaN   \n",
      "233  LAMBORGHINI SESTO ELEMENTO FORZA EDITION FH5 L...      NaN   \n",
      "234                    Lamborghini HuracÃ¡n Performante      NaN   \n",
      "\n",
      "    selftext_sentiment  \n",
      "0                  NaN  \n",
      "1                  NaN  \n",
      "2                  NaN  \n",
      "3                  NaN  \n",
      "4                  NaN  \n",
      "..                 ...  \n",
      "230                NaN  \n",
      "231                NaN  \n",
      "232                NaN  \n",
      "233                NaN  \n",
      "234                NaN  \n",
      "\n",
      "[235 rows x 7 columns]\n",
      "            Word  Frequency  Label\n",
      "0    Lamborghini        208    0.0\n",
      "1      Aventador         29    0.0\n",
      "2          Forza         20    0.0\n",
      "3        Huracan         20    0.0\n",
      "4     Centenario         14    0.0\n",
      "5        HuracÃ¡n         13    0.0\n",
      "6            FH5         11   -1.0\n",
      "7        Horizon         11    0.0\n",
      "8       Countach         10   -1.0\n",
      "9             SV         10    0.0\n",
      "10           SVJ          9   -1.0\n",
      "11   lamborghini          9    1.0\n",
      "12             5          9    1.0\n",
      "13          Urus          7    0.0\n",
      "14   LAMBORGHINI          7    0.0\n",
      "15            LP          7    0.0\n",
      "16        Livery          7    0.0\n",
      "17           new          6    1.0\n",
      "18          Sian          6    0.0\n",
      "19         Super          6    1.0\n",
      "20           car          6    0.0\n",
      "21     Sharecode          6    0.0\n",
      "22        livery          6    0.0\n",
      "23        Diablo          6    0.0\n",
      "24        Veneno          6    1.0\n",
      "25        Trofeo          5    0.0\n",
      "26      favorite          5    0.0\n",
      "27          2016          5    0.0\n",
      "28   Performante          5    0.0\n",
      "29         Sesto          5    1.0\n",
      "30      Elemento          5    1.0\n",
      "31           Evo          5    0.0\n",
      "32    Murcielago          4    1.0\n",
      "33          2020          4    0.0\n",
      "34           FH4          4    0.0\n",
      "35       Essenza          4    0.0\n",
      "36  Lamborghinis          4    0.0\n",
      "37        Series          4    1.0\n",
      "38             J          4    0.0\n",
      "39       Edition          4    0.0\n",
      "40        Espada          4    1.0\n",
      "41      Gallardo          4    0.0\n",
      "42         Miura          4    0.0\n",
      "43           New          4    0.0\n",
      "44        coming          3    0.0\n",
      "45       Ferrari          3    0.0\n",
      "46           STO          3    0.0\n",
      "47    MurciÃ©lago          3    1.0\n",
      "48            GT          3    0.0\n",
      "49      Roadster          3    0.0\n",
      "Topic 1: ['lamborghini', 'huracÃ¡n', 'forza', 'sv', 'horizon']\n",
      "Topic 2: ['horizon', 'forza', 'huracÃ¡n', 'performante', 'race']\n",
      "Topic 3: ['huracÃ¡n', 'performante', 'sto', 'evo', 'fh4']\n",
      "Topic 0: lamborghini + new + car + diablo + free\n",
      "Topic 1: lamborghini + murcielago + livery + inspired + miura\n",
      "Topic 2: lamborghini + centenario + diablo + veneno + murciÃ©lago\n",
      "Topic 3: lamborghini + huracÃ¡n + sto + evo + best\n",
      "Topic 4: lamborghini + fh5 + centenario + elemento + sesto\n",
      "Topic 5: lamborghini + 2020 + essenza + new + brake\n",
      "Topic 6: lamborghini + livery + scv12 + sharecode + essenza\n",
      "Topic 7: lamborghini + aventador + forza + countach + urus\n",
      "Topic 8: lamborghinis + need + bumper + customization + delete\n",
      "Topic 9: lamborghini + huracan + horizon + super + trofeo\n"
     ]
    }
   ],
   "source": [
    "# Set the subreddit and keyword\n",
    "subreddit_names = ['forza']\n",
    "keywords = ['lamborghini', 'Lamborghini', 'Lambo', 'lambo', 'LAMBORGHINI', 'singapore', 'Singapore']\n",
    "# Call the function and get the results\n",
    "posts_results = fetch_posts(subreddit_names, keywords, limit_per_subreddit=None)\n",
    "\n",
    "# Create a data frame\n",
    "posts_df = pd.DataFrame.from_records(posts_results)\n",
    "\n",
    "# Print the data frame\n",
    "print(\"Posts DataFrame:\")\n",
    "print(posts_df)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a DataFrame named 'headlines_df'\n",
    "processed_headlines_df = process_headlines(posts_df, additional_words_to_remove=['lamborghini', 'Lamborghini', 'Lambo', 'lambo', 'LAMBORGHINI', 'livery'])\n",
    "\n",
    "# Display the processed DataFrame\n",
    "print(processed_headlines_df.head(50))\n",
    "\n",
    "# Truncated SVD (LSA) model:\n",
    "result = generate_topics(posts_df, subreddit_name='forza')\n",
    "topics, tfidf_matrix = result[1]  # Access the second element of the tuple\n",
    "\n",
    "# Iterate over topics\n",
    "for topic, top_words in topics.items():\n",
    "    print(f'{topic}: {top_words}')\n",
    "\n",
    "\n",
    "\n",
    "# LDA model:\n",
    "headline_topics = train_lda_model(posts_df['headline'])\n",
    "for topic in headline_topics:\n",
    "    print(topic)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b878950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
